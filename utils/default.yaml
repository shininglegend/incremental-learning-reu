# TA-A-GEM Configuration File
# Default parameters for incremental learning experiments

# BGD Params
mc_iters: 10
std_init: 0.1
mean_eta: 1.0

# Task Configuration
task_type: "permutation" # Options: permutation, rotation, class_split
lite: false # Quick test mode with fewer tasks and data
show_images: false # Whether to show images
output_dir: "./test_results" # Where to put any output files
experiment_name: "Unnamed"

# Dataset Configuration
dataset_name: "mnist" # Dataset to use: mnist or fashion_mnist
num_classes: 10 # For MNIST
input_dim: 784 # For MNIST (28*28)

# Model Configuration
hidden_dim: 200 # Hidden layer dimension as per paper
device: "cuda" # Override to use cuda, we found it was slower on average
random_em: true # Fetch randomly from episodic memory instead of getting all samples

# Memory Configuration
# memory_size_q: 10 # Overridden below
memory_size_p: 3 # Max samples per cluster (P) - as per paper
sampling_rate: 1 # Number of samples per batch to add to memory (0.001 to BATCH_SIZE)
add_remove_randomly: False # Turn on for random cluster assignment and removal

# Training Configuration
batch_size: 10 # Batch size for training (overridden to 50 in lite mode)
learning_rate: 0.001 # Initial learning rate
num_epochs: 20 # Number of epochs per task
# num_tasks: 5 # Number of tasks (overridden to 2 in lite mode)

# Learning Rate Scheduler
use_learning_rate_scheduler: false # Use adaptive learning rate scheduler

# Task Configuration
task_specific:
  permutation:
    num_tasks: 10
    num_pools: 10
    clusters_per_pool: 10
  rotation:
    num_tasks: 10
    num_pools: 10
    clusters_per_pool: 10
  class_split:
    num_tasks: 5
    num_pools: 2
    clusters_per_pool: 50
