def normalize_loss_sliding_window(self, loss_value):
        """Normalize using a sliding window of recent losses"""
        if not hasattr(self, 'loss_history'):
            self.loss_history = []
        
        self.loss_history.append(loss_value)
        
        # Keep only last N losses for normalization bounds
        window_size = 200  # Adjust as needed
        if len(self.loss_history) > window_size:
            self.loss_history = self.loss_history[-window_size:]
        
        if len(self.loss_history) < 2:
            return 0.5  # Default for insufficient data
        
        min_val = min(self.loss_history)
        max_val = max(self.loss_history)

        if min_val == max_val:
            return 0.5
    
        #print(f"Loss value: {loss_value:.6f}, min val: {min_val:.6f}, max val: {max_val:.6f}")

        return (loss_value - min_val) / (max_val - min_val)
    
    def normalize_reference_loss(self, ref_loss):
        """
        Normalize reference loss using the same sliding window approach as current loss.
        This ensures both losses are on the same scale for proper comparison.
        """
        # Use the same loss history for consistent normalization
        if not hasattr(self, 'loss_history') or len(self.loss_history) < 2:
            return 0.5
        
        all_losses = self.loss_history + [ref_loss]
        min_val = min(all_losses)
        max_val = max(all_losses)
        
        if min_val == max_val:
            return 0.5
        
        return (ref_loss - min_val) / (max_val - min_val)



def normalize_loss(self, loss_value):
        """Normalize loss using running min-max normalization"""
        if self.epoch_max_loss == self.epoch_min_loss:
            return 0.5  # Return middle value if all losses are the same
        
        if self.epoch_min_loss == float('inf') or self.epoch_max_loss == float('-inf'):
            return 0.5  # Return middle value if bounds not initialized
        
        normalized = (loss_value - self.epoch_min_loss) / (self.epoch_max_loss - self.epoch_min_loss)
        return max(0.0, min(1.0, normalized))  # Clamp to [0, 1]

### don't really need anymore ###

if current_grad.numel() == 0 or ref_grad.numel() == 0:
            return current_grad

        print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CURRENT LOSS: ", current_loss, " epsilon: ", self.epsilon)

        if current_loss > 0.3:
            # Case 1: Current loss > e
            alpha1 = 1.0
            alpha2 = ref_loss / current_loss if current_loss > 0 else 0.0
        else:
            # We want to be able to add 1 sample per one batch, 
            print("| ========== | IN CASE 2 | ========== |")
            print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CURRENT LOSS: ", current_loss, " epsilon: ", self.epsilon)
            # Case 2: Current loss <= e  
            alpha1 = 0.0
            alpha2 = 1.0
