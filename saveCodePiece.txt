def normalize_loss(self, loss_value):
        """Normalize loss using running min-max normalization"""
        if self.epoch_max_loss == self.epoch_min_loss:
            return 0.5  # Return middle value if all losses are the same
        
        if self.epoch_min_loss == float('inf') or self.epoch_max_loss == float('-inf'):
            return 0.5  # Return middle value if bounds not initialized
        
        normalized = (loss_value - self.epoch_min_loss) / (self.epoch_max_loss - self.epoch_min_loss)
        return max(0.0, min(1.0, normalized))  # Clamp to [0, 1]

### don't really need anymore ###

if current_grad.numel() == 0 or ref_grad.numel() == 0:
            return current_grad

        print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CURRENT LOSS: ", current_loss, " epsilon: ", self.epsilon)

        if current_loss > 0.3:
            # Case 1: Current loss > e
            alpha1 = 1.0
            alpha2 = ref_loss / current_loss if current_loss > 0 else 0.0
        else:
            # We want to be able to add 1 sample per one batch, 
            print("| ========== | IN CASE 2 | ========== |")
            print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CURRENT LOSS: ", current_loss, " epsilon: ", self.epsilon)
            # Case 2: Current loss <= e  
            alpha1 = 0.0
            alpha2 = 1.0
